{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "1_t4V9bnt-hB",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5fbd7903204581d43e0cbd8bf5bd61e8",
     "grade": false,
     "grade_id": "cell-e911fa75d4ae6ea9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "The dataset consists of data about 1000 customers, encompassing 84 features extracted from their financial transactions and current financial status. The main aim is to utilize this dataset for credit risk assessment and forecasting potential defaults.\n",
    "\n",
    "Included within are two target variables, one designed for classification and the other for regression analysis:\n",
    "\n",
    "- **DEFAULT**: Binary target variable indicating if the customer has defaulted (1) or not (0)\n",
    "- **CREDIT_SCORE**: Numerical target variable representing the customer's credit score (integer)\n",
    "\n",
    "and these features:\n",
    "\n",
    "- **INCOME**: Total income in the last 12 months\n",
    "- **SAVINGS**: Total savings in the last 12 months\n",
    "- **DEBT**: Total existing debt\n",
    "- **R_SAVINGS_INCOME**: Ratio of savings to income\n",
    "- **R_DEBT_INCOME**: Ratio of debt to income\n",
    "- **R_DEBT_SAVINGS**: Ratio of debt to savings\n",
    "\n",
    "Transaction groups (**GROCERIES**, **CLOTHING**, **HOUSING**, **EDUCATION**, **HEALTH**, **TRAVEL**, **ENTERTAINMENT**, **GAMBLING**, **UTILITIES**, **TAX**, **FINES**) are categorized.\n",
    "\n",
    "- **T_{GROUP}_6**: Total expenditure in that group in the last 6 months\n",
    "- **T_GROUP_12**: Total expenditure in that group in the last 12 months\n",
    "- **R_[GROUP]**: Ratio of T_[GROUP]6 to T[GROUP]_12\n",
    "- **R_[GROUP]INCOME**: Ratio of T[GROUP]_12 to INCOME\n",
    "- **R_[GROUP]SAVINGS**: Ratio of T[GROUP]_12 to SAVINGS\n",
    "- **R_[GROUP]DEBT**: Ratio of T[GROUP]_12 to DEBT\n",
    "\n",
    "Categorical Features:\n",
    "\n",
    "- **CAT_GAMBLING**: Gambling category (none, low, high)\n",
    "- **CAT_DEBT**: 1 if the customer has debt; 0 otherwise\n",
    "- **CAT_CREDIT_CARD**: 1 if the customer has a credit card; 0 otherwise\n",
    "- **CAT_MORTGAGE**: 1 if the customer has a mortgage; 0 otherwise\n",
    "- **CAT_SAVINGS_ACCOUNT**: 1 if the customer has a savings account; 0 otherwise\n",
    "- **CAT_DEPENDENTS**: 1 if the customer has any dependents; 0 otherwise\n",
    "- **CAT_LOCATION**: Location (San Francisco, Philadelphia, Los Angeles, etc.)\n",
    "- **CAT_MARITAL_STATUS**: Marital status (Married, Widowed, Divorced or Single)\n",
    "- **CAT_EDUCATION**: Level of Education (Postgraduate, College, High School or Graduate)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T13:13:43.396554250Z",
     "start_time": "2024-02-01T13:13:43.012460676Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7871551e55faf200ca6bfe8177b9d4b1",
     "grade": false,
     "grade_id": "cell-0d9a7d4e70c072c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(transform_output=\"pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e0f7fa; border-left: 5px solid #0097a7; padding: 10px; color: #005662;\">\n",
    "    <h2 style=\"color: #333;\">Guidance through the Notebook</h2>\n",
    "    <p>We start by reading the data and preprocessing it in a very simple way (as the aim of the project is to focus on models and gridsearch) making train-test split again.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T13:14:04.097769079Z",
     "start_time": "2024-02-01T13:13:44.161097827Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "174ff7858c0bdac5a8a8a69899540683",
     "grade": true,
     "grade_id": "ex6",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import set_config\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/jnin/information-systems/main/data/AI2_23_24_credit_score.csv\"\n",
    "\n",
    "# create the dataframe\n",
    "df = pd.read_csv(url)\n",
    "df = df.drop('CUST_ID', axis=1)\n",
    "df.head()\n",
    "\n",
    "# we reget the original df with the CREDIT_SCORE column\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/00/r_n6l1n57999hdn7w1qw3nnc0000gn/T/ipykernel_462/4011445831.py:17: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  correlation_matrix = df.corr()\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing\n",
    "\n",
    "# Dropping the CUST_ID column\n",
    "df = df.drop('CUST_ID', axis=1)\n",
    "\n",
    "# We can print the correlation plot and observe some correlations across columns (we comment it as too many columns are printed)\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "# plt.show()\n",
    "# print(df.corr())\n",
    "\n",
    "\n",
    "# get columns correlated >0.9 with each other\n",
    "correlated_features = set()\n",
    "correlation_matrix = df.corr()\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.9:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "correlated_features\n",
    "# Drop the correlated columns\n",
    "df.drop(columns=correlated_features, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['CREDIT_SCORE']\n",
    "X = df.drop('CREDIT_SCORE', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25, random_state=25)\n",
    "# Creation of pipeline and transformers\n",
    "categorical_features = X_train.select_dtypes(include = ['object']).columns.tolist()\n",
    "numerical_features = X_train.select_dtypes(include = ['int64', 'float64']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e0f7fa; border-left: 5px solid #0097a7; padding: 10px; color: #005662;\">\n",
    "    <h2 style=\"color: #333;\">Approach 1</h2>\n",
    "    <p>The first approach consisted on running a gridsearch only on the 2 models and their hyperparameters but not on the data preprocessing for numerical and categorical data, so although this approach is suboptimal, we will use it as a starting point.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('num',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer()),\n",
      "                                                                  ('scaler',\n",
      "                                                                   StandardScaler())]),\n",
      "                                                  ['INCOME', 'SAVINGS', 'DEBT',\n",
      "                                                   'R_SAVINGS_INCOME',\n",
      "                                                   'R_DEBT_INCOME',\n",
      "                                                   'R_DEBT_SAVINGS',\n",
      "                                                   'T_CLOTHING_12',\n",
      "                                                   'R_CLOTHING',\n",
      "                                                   'R_CLOTHING_INCOME',\n",
      "                                                   'R_CLOTHING_SAVINGS',\n",
      "                                                   'R_CLOTHING_DEBT',\n",
      "                                                   'T_EDUCATION_12',\n",
      "                                                   'R_EDUCATION',\n",
      "                                                   'R_EDUC...\n",
      "                              feature_types=None, gamma=None, gpu_id=None,\n",
      "                              grow_policy=None, importance_type=None,\n",
      "                              interaction_constraints=None, learning_rate=None,\n",
      "                              max_bin=None, max_cat_threshold=None,\n",
      "                              max_cat_to_onehot=None, max_delta_step=None,\n",
      "                              max_depth=3, max_leaves=None,\n",
      "                              min_child_weight=None, missing=nan,\n",
      "                              monotone_constraints=None, n_estimators=100,\n",
      "                              n_jobs=None, num_parallel_tree=None,\n",
      "                              predictor=None, random_state=25, ...))])\n",
      "Best parameters: {'regressor': XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=25, ...), 'regressor__max_depth': 3, 'regressor__n_estimators': 100}\n",
      "Best score on training set: -892.70993259761\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "## Approach 1: Defining the pipeline and conducting gridsearch only for the regressor models\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipe, numerical_features),\n",
    "        ('cat', categorical_pipe, categorical_features)])\n",
    "\n",
    "# Placeholder for the regression model\n",
    "regression_model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                   ('regressor', None)])  # 'regressor' will be defined in the grid search\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# import mean_squared_error from sklearn.metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Parameter grid for grid search, including both regressors and potential preprocessing steps\n",
    "param_grid = [{\n",
    "    'regressor': [XGBRegressor(random_state=25)],\n",
    "    'regressor__n_estimators': [100, 200],\n",
    "    'regressor__max_depth': [3, 4, 5],\n",
    "    # Add more parameters specific to XGBRegressor\n",
    "}, {\n",
    "    'regressor': [SVR()],\n",
    "    'regressor__C': [0.1, 1, 10],\n",
    "    'regressor__epsilon': [0.01, 0.1, 0.5],\n",
    "    # Add more parameters specific to SVR\n",
    "}]\n",
    "\n",
    "# We optimise for negative mean squared error as scikit-learn optimises for maximising the score.\n",
    "grid_search = GridSearchCV(regression_model, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"Best model: {best_model}\")\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best score on training set: {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e0f7fa; border-left: 5px solid #0097a7; padding: 10px; color: #005662;\">\n",
    "    <h2 style=\"color: #333;\">Approach 2</h2>\n",
    "    <p>Approach 2 tries to tackle the last problem better, by applying preprocessing steps inside the gridsearch so we can seek for more optimal solutions</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('num',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer(strategy='most_frequent')),\n",
      "                                                                  ('scaler',\n",
      "                                                                   MinMaxScaler())]),\n",
      "                                                  ['INCOME', 'SAVINGS', 'DEBT',\n",
      "                                                   'R_SAVINGS_INCOME',\n",
      "                                                   'R_DEBT_INCOME',\n",
      "                                                   'R_DEBT_SAVINGS',\n",
      "                                                   'T_CLOTHING_12',\n",
      "                                                   'R_CLOTHING',\n",
      "                                                   'R_CLOTHING_INCOME',\n",
      "                                                   'R_CLOTHING_SAVINGS',\n",
      "                                                   'R_CLOTHING_DEBT',\n",
      "                                                   'T_EDUCATION_12'...\n",
      "                              feature_types=None, gamma=None, gpu_id=None,\n",
      "                              grow_policy=None, importance_type=None,\n",
      "                              interaction_constraints=None, learning_rate=0.1,\n",
      "                              max_bin=None, max_cat_threshold=None,\n",
      "                              max_cat_to_onehot=None, max_delta_step=None,\n",
      "                              max_depth=3, max_leaves=None,\n",
      "                              min_child_weight=None, missing=nan,\n",
      "                              monotone_constraints=None, n_estimators=100,\n",
      "                              n_jobs=None, num_parallel_tree=None,\n",
      "                              predictor=None, random_state=25, ...))])\n",
      "Best parameters: {'preprocessor__cat__onehot__drop': 'first', 'preprocessor__num__imputer__strategy': 'most_frequent', 'preprocessor__num__scaler': MinMaxScaler(), 'regressor': XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=25, ...), 'regressor__learning_rate': 0.1, 'regressor__max_depth': 3, 'regressor__n_estimators': 100}\n",
      "Best score on training set: -772.4670440071845\n"
     ]
    }
   ],
   "source": [
    "## Approach 2: Defining the pipeline and conducting gridsearch for both the regressor and the preprocessing steps\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer()), # Strategy will be defined in grid search\n",
    "    ('scaler', StandardScaler())]) # Scaler will be defined in grid search\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), # Strategy can also be defined in grid search\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))]) # Parameters will be defined in grid search\n",
    "\n",
    "# Transformer for preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipe, numerical_features),\n",
    "        ('cat', categorical_pipe, categorical_features)])\n",
    "\n",
    "# Placeholder for the regression model\n",
    "regression_model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                   ('regressor', None)])  # 'regressor' will be defined in the grid search\n",
    "\n",
    "# Parameter grid for grid search, now including preprocessing steps\n",
    "param_grid = [{\n",
    "    'preprocessor__num__imputer__strategy': ['mean', 'median', 'most_frequent'], # Define strategies for SimpleImputer\n",
    "    'preprocessor__num__scaler': [StandardScaler(), MinMaxScaler()], # Define scalers\n",
    "    'preprocessor__cat__onehot__drop': ['first', None], # Define drop parameter for OneHotEncoder\n",
    "    'regressor': [XGBRegressor(random_state=25)], # Define the regressor\n",
    "    'regressor__n_estimators': [100, 200, 300], # n estimators 100-300 will specify number of trees in the forest\n",
    "    'regressor__max_depth': [3, 4, 5], # max depth 3-5 will specify the maximum depth of the tree\n",
    "    'regressor__learning_rate': [0.01, 0.1, 0.2] # learning rate will specify the step size at each iteration while moving toward a minimum of a loss function\n",
    "}, {\n",
    "    'preprocessor__num__imputer__strategy': ['mean', 'median', 'most_frequent'], # Define strategies for SimpleImputer\n",
    "    'preprocessor__num__scaler': [StandardScaler(), MinMaxScaler()], # Define scalers\n",
    "    'preprocessor__cat__onehot__drop': ['first', None], # Define drop parameter for OneHotEncoder\n",
    "    'regressor': [SVR()], # Define the regressor\n",
    "    'regressor__C': [0.1, 1, 10],   # C is a regularization parameter\n",
    "                                    # Smaller C mean a more regularized model, larger C mean a less regularized model\n",
    "    'regressor__epsilon': [0.01, 0.1], # Epsilon is the width of the street\n",
    "    'regressor__kernel': ['linear', 'poly', 'rbf', 'sigmoid'] # Kernel specifies the kernel type to be used in the algorithm\n",
    "}]\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(regression_model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-2) # n_jobs=-2 means that all CPUs but one are used, needed as it took long (30')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"Best model: {best_model}\")\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best score on training set: {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e0f7fa; border-left: 5px solid #0097a7; padding: 10px; color: #005662;\">\n",
    "    <h2 style=\"color: #333;\">Approach 1 & 2: Model Optimization Summary</h2>\n",
    "    <p>Our refined grid search did not shifte our top model preference to <strong>SVR</strong> from XGBoost, XGBoost was still our best performing model. This approach fine-tuned our preprocessing strategy, employing <code>mean</code> imputation and <code>StandardScaler</code> for numerical data, and <code>most_frequent</code> imputation with <code>OneHotEncoder</code> for categorical data.</p>\n",
    "    <p>Optimal SVR parameters were identified as <strong>C=1</strong> and <strong>Îµ=1</strong>, favoring a linear kernel, which proved effective for our dataset's dimensional characteristics.</p>\n",
    "    <p>The score on the training set was improved from -858 to -787.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e0f7fa; border-left: 5px solid #0097a7; padding: 10px; color: #005662;\">\n",
    "    <h2 style=\"color: #333;\">Approach 3</h2>\n",
    "    <p>The third approach now tries to improve the last gridsearch by <strong>introducing 2 more models</strong> (with its hyperparameters too) inside of it. We will now compute the gridsearch with XGBoost, SVR, RandomForest, and GradientBoosting.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('num',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer(strategy='most_frequent')),\n",
      "                                                                  ('scaler',\n",
      "                                                                   MinMaxScaler())]),\n",
      "                                                  ['INCOME', 'SAVINGS', 'DEBT',\n",
      "                                                   'R_SAVINGS_INCOME',\n",
      "                                                   'R_DEBT_INCOME',\n",
      "                                                   'R_DEBT_SAVINGS',\n",
      "                                                   'T_CLOTHING_12',\n",
      "                                                   'R_CLOTHING',\n",
      "                                                   'R_CLOTHING_INCOME',\n",
      "                                                   'R_CLOTHING_SAVINGS',\n",
      "                                                   'R_CLOTHING_DEBT',\n",
      "                                                   'T_EDUCATION_12'...\n",
      "                                                   'R_FINES_DEBT',\n",
      "                                                   'T_GAMBLING_12',\n",
      "                                                   'R_GAMBLING',\n",
      "                                                   'R_GAMBLING_INCOME', ...]),\n",
      "                                                 ('cat',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer(strategy='most_frequent')),\n",
      "                                                                  ('onehot',\n",
      "                                                                   OneHotEncoder(drop='first',\n",
      "                                                                                 handle_unknown='ignore',\n",
      "                                                                                 sparse_output=False))]),\n",
      "                                                  ['CAT_GAMBLING',\n",
      "                                                   'CAT_LOCATION',\n",
      "                                                   'CAT_MARITAL_STATUS',\n",
      "                                                   'CAT_EDUCATION'])])),\n",
      "                ('regressor', GradientBoostingRegressor(random_state=25))])\n",
      "Best parameters: {'preprocessor__cat__onehot__drop': 'first', 'preprocessor__num__imputer__strategy': 'most_frequent', 'preprocessor__num__scaler': MinMaxScaler(), 'regressor': GradientBoostingRegressor(random_state=25), 'regressor__learning_rate': 0.1, 'regressor__max_depth': 3, 'regressor__n_estimators': 100}\n",
      "Best score on training set: -756.7979296363086\n"
     ]
    }
   ],
   "source": [
    "# Approach 3: More complex approach (exploratory analysis): Comparing 4 models: XGBoost, SVR, RandomForest, and GradientBoosting and their parameters\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer()),  # Strategy will be defined in grid search\n",
    "    ('scaler', StandardScaler())])  # Scaler will be defined in grid search\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Strategy can also be defined in grid search\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])  # Parameters will be defined in grid search\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numerical_pipe, numerical_features),\n",
    "    ('cat', categorical_pipe, categorical_features)])\n",
    "\n",
    "# Placeholder for the regression model\n",
    "regression_model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                   ('regressor', None)])  # 'regressor' will be defined in the grid search\n",
    "\n",
    "# Parameter grid for grid search, now including additional models\n",
    "# Comments on models used before are not going to be explained in comments again (Check comments in Approach 2).\n",
    "param_grid = [\n",
    "    {\n",
    "        'preprocessor__num__imputer__strategy': ['mean', 'median', 'most_frequent'],\n",
    "        'preprocessor__num__scaler': [StandardScaler(), MinMaxScaler()],\n",
    "        'preprocessor__cat__onehot__drop': ['first', None],\n",
    "        'regressor': [XGBRegressor(random_state=25)], #random_state=25 for reproducibility\n",
    "        'regressor__n_estimators': [100, 200, 400],\n",
    "        'regressor__max_depth': [3, 4, 5],\n",
    "        'regressor__learning_rate': [0.01, 0.1]\n",
    "    },\n",
    "    {\n",
    "        'preprocessor__num__imputer__strategy': ['mean', 'median', 'most_frequent'],\n",
    "        'preprocessor__num__scaler': [StandardScaler(), MinMaxScaler()],\n",
    "        'preprocessor__cat__onehot__drop': ['first', None],\n",
    "        'regressor': [SVR()],\n",
    "        'regressor__C': [0.1, 1, 10],\n",
    "        'regressor__epsilon': [0.01, 0.1],\n",
    "        'regressor__kernel': ['linear', 'poly', 'rbf']\n",
    "    },\n",
    "    {\n",
    "        'preprocessor__num__imputer__strategy': ['mean', 'median', 'most_frequent'],\n",
    "        'preprocessor__num__scaler': [StandardScaler(), MinMaxScaler()],\n",
    "        'preprocessor__cat__onehot__drop': ['first', None],\n",
    "        'regressor': [RandomForestRegressor(random_state=25)],\n",
    "        'regressor__n_estimators': [100, 200, 400],\n",
    "        'regressor__max_depth': [3, 4, 5]\n",
    "    },\n",
    "    {\n",
    "        'preprocessor__num__imputer__strategy': ['mean', 'median', 'most_frequent'],\n",
    "        'preprocessor__num__scaler': [StandardScaler(), MinMaxScaler()],\n",
    "        'preprocessor__cat__onehot__drop': ['first', None],\n",
    "        'regressor': [GradientBoostingRegressor(random_state=25)],\n",
    "        'regressor__n_estimators': [100, 200, 400],\n",
    "        'regressor__learning_rate': [0.01, 0.1],\n",
    "        'regressor__max_depth': [3, 4, 5]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(regression_model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"Best model: {best_model}\")\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best score on training set: {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e0f7fa; border-left: 5px solid #0097a7; padding: 10px; color: #005662;\">\n",
    "    <h3 style=\"color: #333;\">Approach 3 Gridsearch Expansion Reflections</h3>\n",
    "    <p>Starting with SVR and XGBRegressor, we later included RandomForestRegressor and GradientBoostingRegressor in our model exploration. The inclusion of these 2 new models in the search, resulted in GradientBoosting overperforming the other three models. We are now getting a training score of <strong>-756.</strong></p>\n",
    "    <p><strong>Key Takeaway:</strong></p>\n",
    "    <ul>\n",
    "        <li>Gridsearch with more models help us evaluate and compare different models and hyperparameters on the training set.</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e0f7fa; border-left: 5px solid #0097a7; padding: 10px; color: #005662;\">\n",
    "    <h3 style=\"color: #333;\">Approach 4: Fine-tuning our best model</h3>\n",
    "    <p>As we know that GradientBoosting is our best-performing model, we will try to improve it by fine-tuning the \"optimal parameters\", which we believe they are still suboptimal. This will be done by conducting a gridsearch in the GradientBoosting model but we will include parameters to be tested that have to be very close to the ones selected by our latest gridsearch.</p>\n",
    "    <p>This approach will make our GradientBoosting at least slightly better (no worse at all) as the last model will also be included in the new gridsearch.</p>\n",
    "    <p>Thisway we can get more optimal values for number of trees per forest, maximum depth and learning rate.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('num',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer()),\n",
      "                                                                  ('scaler',\n",
      "                                                                   MinMaxScaler())]),\n",
      "                                                  ['INCOME', 'SAVINGS', 'DEBT',\n",
      "                                                   'R_SAVINGS_INCOME',\n",
      "                                                   'R_DEBT_INCOME',\n",
      "                                                   'R_DEBT_SAVINGS',\n",
      "                                                   'T_CLOTHING_12',\n",
      "                                                   'R_CLOTHING',\n",
      "                                                   'R_CLOTHING_INCOME',\n",
      "                                                   'R_CLOTHING_SAVINGS',\n",
      "                                                   'R_CLOTHING_DEBT',\n",
      "                                                   'T_EDUCATION_12',\n",
      "                                                   'R_EDUCATION',\n",
      "                                                   'R_EDUCAT...\n",
      "                                                   'R_GAMBLING_INCOME', ...]),\n",
      "                                                 ('cat',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer(strategy='most_frequent')),\n",
      "                                                                  ('onehot',\n",
      "                                                                   OneHotEncoder(drop='first',\n",
      "                                                                                 handle_unknown='ignore',\n",
      "                                                                                 sparse_output=False))]),\n",
      "                                                  ['CAT_GAMBLING',\n",
      "                                                   'CAT_LOCATION',\n",
      "                                                   'CAT_MARITAL_STATUS',\n",
      "                                                   'CAT_EDUCATION'])])),\n",
      "                ('regressor',\n",
      "                 GradientBoostingRegressor(learning_rate=0.09, n_estimators=110,\n",
      "                                           random_state=25))])\n",
      "Best parameters: {'preprocessor__cat__onehot__drop': 'first', 'preprocessor__num__imputer__strategy': 'mean', 'preprocessor__num__scaler': MinMaxScaler(), 'regressor': GradientBoostingRegressor(learning_rate=0.09, n_estimators=110, random_state=25), 'regressor__learning_rate': 0.09, 'regressor__max_depth': 3, 'regressor__n_estimators': 110}\n",
      "Best score on training set: -754.799775884036\n",
      "Generalization score: -924.61417222603\n"
     ]
    }
   ],
   "source": [
    "# Approach 4 Gridsearch for GradientBoosting specifically \n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer()),  # Strategy will be defined in grid search\n",
    "    ('scaler', StandardScaler())])  # Scaler will be defined in grid search\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Strategy can also be defined in grid search\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])  # Parameters will be defined in grid search\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numerical_pipe, numerical_features),\n",
    "    ('cat', categorical_pipe, categorical_features)])\n",
    "\n",
    "# Placeholder for the regression model\n",
    "regression_model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                   ('regressor', None)])  # 'regressor' will be defined in the grid search\n",
    "\n",
    "# Parameter grid for grid search, now including additional models\n",
    "param_grid = [\n",
    "    {\n",
    "        'preprocessor__num__imputer__strategy': ['mean', 'median', 'most_frequent'],\n",
    "        'preprocessor__num__scaler': [StandardScaler(), MinMaxScaler()],\n",
    "        'preprocessor__cat__onehot__drop': ['first', None],\n",
    "        'regressor': [GradientBoostingRegressor(random_state=25)],\n",
    "        'regressor__n_estimators': [80,90,100,110,120], #wider range than Approach 3\n",
    "        'regressor__learning_rate': [0.09,0.1,0.11], #wider range than Approach 3\n",
    "        'regressor__max_depth': [1,2,3,4] #wider range than Approach 3\n",
    "    },\n",
    "]\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(regression_model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"Best model: {best_model}\")\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best score on training set: {best_score}\")\n",
    "\n",
    "\n",
    "# We make prediction using the best model of the loop found through the grid search.\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# And then, we calculate the Mean Squared Error of the predictions on the test set.\n",
    "mse_test = - mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Finally, we print the generalization score (MSE on the test set)\n",
    "print(f\"Generalization score: {mse_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e0f7fa; border-left: 5px solid #0097a7; padding: 10px; color: #005662;\">\n",
    "    <h3 style=\"color: #333;\">Approach 4 conclusions: Streamlined Model Optimization Summary</h3>\n",
    "    <p>After identifying GradientBoosting regressor as our model of choice, we refined its hyperparameters by conducting a focused grid search, exploring n values [80,90,100,110,120], max depth values [1,2,3,4] and diferent learning rates [0.09,0.1,0.11].</p>\n",
    "    <p><strong>Results:</strong> This led to an optimized GradientBoosting model with <strong>n=110, maximum depth=3 and learning rate of0.09 </strong>, which showcased a better nMSE of <strong>-755</strong> on the validation set. Therefore, becoming our best-performing model out of all the ones we tried.</p>\n",
    "    <p>Only now that the model has been defined and selected, we can get the generalization score by using the test set. Even though it is worse than the validation score, as expected, our generalisation score is a promising negative mean squared error -924.</p>\n",
    "    <p><strong>Takeaway:</strong> Fine-tuning confirmed the effectiveness of precision in hyperparameter optimization, enhancing our model's accuracy for deployment. Only in this last step, once the model has been clearly defined, have we used the test set in our code.</p>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Session I.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
